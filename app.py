import streamlit as st
from langchain_community.chat_models import ChatZhipuAI
import uuid
import socket
import json
import redis
import os
from dotenv import load_dotenv

load_dotenv()
# Redis å®¢æˆ·ç«¯é…ç½®ï¼ˆç”¨äºå³æ—¶æ›´æ–°å†å²ï¼‰
redis_client = redis.Redis(host="localhost", port=6379, db=0)

# LangChain LLM åˆå§‹åŒ–
ZHIPU_API_KEY = os.getenv('ZHIPU_API_KEY')

# LangChain LLM åˆå§‹åŒ–
llm = ChatZhipuAI(
    api_key=ZHIPU_API_KEY,
    temperature=0.5,
    model="glm-4-flash",
)


def get_history():
    try:
        keys = redis_client.keys("conversation:*")
        history = []
        for key in keys:
            convo = redis_client.hgetall(key)
            history.append(
                {
                    "conversation_id": key.decode().split(":")[1],
                    "messages": json.loads(convo.get(b"messages", b"[]").decode()),
                }
            )
        # æŒ‰æ—¶é—´æ’åºæˆ–å…¶ä»–é€»è¾‘
        return history
    except Exception as e:
        st.sidebar.error(f"Error fetching history: {e}")
        return []


def publish_message(message, host="localhost", port=9999):
    try:
        client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        client.connect((host, port))
        client.sendall("PUBLISHER\n".encode())
        # Send the message as JSON string
        client.sendall((json.dumps(message)+'\n').encode())
        client.close()
        print("Published message to Message Broker.")
    except Exception as e:
        st.error(f"Error publishing message: {e}")

def page_control(conversation_id):
    if conversation_id:
        # ä» Redis è·å–å¯¹è¯
        try:
            redis_key = f"conversation:{conversation_id}"
            convo = redis_client.hgetall(redis_key)
            user_query = convo.get(b"user_query", b"").decode()
            ai_response = convo.get(b"ai_response", b"").decode()
            return user_query, ai_response
        except Exception as e:
            st.error(f"Error fetching conversation: {e}")
            st.session_state.logs.append(f"Error fetching conversation: {e}")
    else:
        st.session_state.conversation_id = str(uuid.uuid4())
        return None, None

st.title("AI é—®ç­”ç³»ç»Ÿ")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "conversation_id" not in st.session_state:
    st.session_state.conversation_id = str(uuid.uuid4())
if "logs" not in st.session_state:
    st.session_state.logs = []

# ä¾§è¾¹æ æ˜¾ç¤ºå†å²å¯¹è¯
st.sidebar.header("å†å²å¯¹è¯")

history = get_history()
for convo in history:
    if st.sidebar.button(f"å¯¹è¯ {convo['conversation_id']}", key=convo['conversation_id'],use_container_width=True):
        st.session_state.conversation_id = convo['conversation_id']
        st.session_state.messages = []
        for messages in convo['messages']:
            st.session_state.messages.append(messages)

if st.sidebar.button("æ–°å»ºå¯¹è¯", use_container_width=True):
    st.session_state.conversation_id = str(uuid.uuid4())
    st.session_state.messages = []

for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message(message["role"], avatar="â˜ºï¸"):
            st.markdown(message["content"])
    else:
        with st.chat_message(message["role"], avatar="ğŸ¤–"):
            st.markdown(message["content"])

if prompt := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜"):
    with st.chat_message("user", avatar="â˜ºï¸"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    try:
        response = llm.invoke(prompt)
    except Exception as e:
        st.error(f"AI ç”Ÿæˆå“åº”å¤±è´¥: {e}")
        
    with st.chat_message('assistant', avatar='ğŸ¤–'):
        st.markdown(response.content)
    st.session_state.messages.append({'role': 'assistant', 'content': response.content})

# # ç”¨æˆ·è¾“å…¥
# user_input = st.text_input(f"{st.session_state.conversation_id}", "")

# if st.button("å‘é€") and user_input:
#     # ç”Ÿæˆå”¯ä¸€çš„ conversation_id
#     conversation_id = str(uuid.uuid4())

#     # è°ƒç”¨ LLM è·å–å“åº”
#     try:
#         response = llm.invoke(user_input)
#     except Exception as e:
#         st.error(f"AI ç”Ÿæˆå“åº”å¤±è´¥: {e}")
#         response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå“åº”ã€‚"
#     content = response.content
#     # æ˜¾ç¤º AI å“åº”
#     st.write("**AI**:", content)

    # è®¡ç®— token ä½¿ç”¨é‡ï¼ˆç¤ºä¾‹ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    tokens_used = int(response.response_metadata['token_usage']['total_tokens'])

    #TODO æ¶ˆæ¯è¿½åŠ çš„æ—¶å€™å‡ºç°é‡å¤
    # åˆ›å»ºæ¶ˆæ¯
    conversation = {
        "conversation_id": st.session_state.conversation_id,
        "messages": st.session_state.messages,
        "tokens_used": tokens_used,
        "logs":st.session_state.logs
    }

    # å‘å¸ƒæ¶ˆæ¯åˆ°ä¸­é—´ä»¶
    publish_message(conversation)

    # # æ›´æ–° Redis ç«‹å³æ˜¾ç¤º
    # try:
    #     redis_key = f"conversation:{conversation_id}"
    #     redis_client.hmset(
    #         redis_key, {"user_query": user_input, "ai_response": content}
    #     )
    # except Exception as e:
    #     st.error(f"Error updating history: {e}")
